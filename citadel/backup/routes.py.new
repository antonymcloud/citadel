@backup_bp.route('/api/repository/<int:repo_id>/growth-chart')
@login_required
def api_repository_growth_chart(repo_id):
    """API endpoint to get repository growth chart data"""
    repository = Repository.query.get_or_404(repo_id)
    
    # Security check - make sure the repository belongs to the current user
    if repository.user_id != current_user.id:
        return jsonify({'error': 'Permission denied'}), 403
    
    # Get all successful backup jobs for this repository
    backup_jobs = Job.query.filter_by(
        repository_id=repo_id, 
        job_type='create', 
        status='success'
    ).order_by(Job.timestamp.asc()).all()
    
    # Need at least 2 data points for a meaningful chart
    if len(backup_jobs) < 2:
        # Generate sample data for client-side rendering
        sample_dates = [
            (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d %H:%M'),
            (datetime.now() - timedelta(days=25)).strftime('%Y-%m-%d %H:%M'),
            (datetime.now() - timedelta(days=20)).strftime('%Y-%m-%d %H:%M'),
            (datetime.now() - timedelta(days=15)).strftime('%Y-%m-%d %H:%M'),
            (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d %H:%M'),
            (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d %H:%M'),
            datetime.now().strftime('%Y-%m-%d %H:%M')
        ]
        sample_sizes = [
            "1.2 GB", 
            "1.5 GB", 
            "1.8 GB", 
            "2.2 GB", 
            "2.5 GB", 
            "2.8 GB", 
            "3.0 GB"
        ]
        sample_labels = [f"Sample Backup {i+1}" for i in range(len(sample_dates))]
        
        # Format for client-side rendering
        return jsonify({
            'growth_data': {
                'labels': sample_dates,
                'data': [1.2, 1.5, 1.8, 2.2, 2.5, 2.8, 3.0]
            },
            'is_sample_data': True,
            'message': 'Showing sample data. Create at least 2 backups to see actual growth.'
        })
    
    # Extract data from the jobs
    growth_data = []
    
    # Helper function to extract size from job metadata
    def extract_job_size(job):
        if not job.get_metadata() or 'stats' not in job.get_metadata():
            return None
            
        stats = job.get_metadata().get('stats', {})
        
        # First try to get deduplicated size
        if 'this_archive_deduplicated_size' in stats:
            size_str = stats['this_archive_deduplicated_size']
            try:
                from citadel.backup.utils import parse_size
                # Convert to GB for consistent charting
                size_bytes = parse_size(size_str)
                return size_bytes / (1024 * 1024 * 1024)  # Convert bytes to GB
            except (ValueError, IndexError) as e:
                print(f"DEBUG: Error parsing size: {e}")
                
        return None
    
    # Collect all valid data points
    dates = []
    sizes = []
    labels = []
    
    for job in backup_jobs:
        size = extract_job_size(job)
        
        if size is not None:
            dates.append(job.timestamp.strftime('%Y-%m-%d %H:%M'))
            sizes.append(round(size, 2))  # Round to 2 decimal places
            labels.append(job.archive_name or f"Backup {job.id}")
    
    # Need at least 2 valid data points for a chart
    if len(dates) < 2:
        return jsonify({
            'growth_data': {
                'labels': sample_dates,
                'data': [1.2, 1.5, 1.8, 2.2, 2.5, 2.8, 3.0]
            },
            'is_sample_data': True,
            'message': 'Showing sample data. Valid size information not found in existing backups.'
        })
    
    # Return the growth chart data
    return jsonify({
        'growth_data': {
            'labels': dates,
            'data': sizes,
            'archive_names': labels
        },
        'is_sample_data': False
    })
